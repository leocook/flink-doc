# 01 - Flink DataStream API编程手册

Flink是实现分布式数据集转换（例如：filtering, mapping, updating state, joining, grouping, defining windows, aggregating）的常规程序。初始化数据集的方式有多种，例如：从本地文件、Kafka的topic、或内存中读取数据等等。结果通过sinks来返回，sinks可以把数据写入到（分布式）文件系统中，或者标准输出等等。Flink程序可以运行在多种环境中，可以是standalone，或者嵌入式运行在其它程序中。程序可以执行在本地JVM中，也可以在多台机器组成的集群上运行。

在阅读本章之前，可以查看一下上一章Flink API的一些基础概念。

为了能够创建你自己的Flink DataStream程序，我们鼓励开发者能开始解剖Flink程序，并且逐步添加自己的转换操作。剩下的章节将会逐渐展示其它操作和高级特性。

## 样例程序
下面的程序使用Flink实现了基于窗口的word count流式计算，是一个完整的可运行的程序例子，数据是从web socket中获取的，每5秒一个窗口。你可以把代码拷贝到本地来运行：

- Java

```
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

public class WindowWordCount {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Tuple2<String, Integer>> dataStream = env
                .socketTextStream("localhost", 9999)
                .flatMap(new Splitter())
                .keyBy(0)
                .timeWindow(Time.seconds(5))
                .sum(1);

        dataStream.print();

        env.execute("Window WordCount");
    }

    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {
        @Override
        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {
            for (String word: sentence.split(" ")) {
                out.collect(new Tuple2<String, Integer>(word, 1));
            }
        }
    }

}
```

- Scala

```
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

object WindowWordCount {
  def main(args: Array[String]) {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val text = env.socketTextStream("localhost", 9999)

    val counts = text.flatMap { _.toLowerCase.split("\\W+") filter { _.nonEmpty } }
      .map { (_, 1) }
      .keyBy(0)
      .timeWindow(Time.seconds(5))
      .sum(1)

    counts.print()

    env.execute("Window Stream WordCount")
  }
}
```

在运行程序之前，先在命令行下使用netcat启动一个输入流:

```
nc -lk 9999
```

只需要输入一下单次并回车就可以返回一个新的词了。这将会是word count程序的输入。如果你想获取word count值大于1，那么久需要在5秒内要输入相同的单词。

## 数据源（Data Sources）
Sources也就是你的程序读数据时的输入。你可以使用StreamExecutionEnvironment.addSource(sourceFunction)将数据源绑定到程序中。Flink附带了许多预先实现的源函数，但是你也可以通过实现SourceFunction接口来定义自己的非并行数据源，或者通过实现ParallelSourceFunction接口或继承RichParallelSourceFunction类来定义自己的并行数据源。

这里有多个可以通过StreamExecutionEnvironment得到的数据源：

### 基于文件的数据源
- readTextFile(path) - 读取文本文件，即每一行都遵循TextInputFormat规范的文件，并将它们作为字符串返回。

- readFile(fileInputFormat, path) - 按照指定的FileInputFormat来读取（一次）文件。

- readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是前两个方法在内部调用的方法。它可以指定path对应的文件格式（fileInputFormat）。source可以根据watchType，来周期性（每interval毫秒）的监控路径中的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者只处理一次当前路径中的数据并退出。使用pathFilter可以进一步过滤掉处理文件。

具体实现:

在内部，Flink把文件读取操作分为2个子任务，即目录监控和数据读取。这两个子任务都是被单独实现的。目录监控不是并发的，但是数据读取是被多个任务并行运行的。其中数据读取任务的并发数也就是Job的并发度。监控任务也就是扫描目录，具体是周期性的扫描还是指扫描一次取决于配置的watchType，发现将要被处理的文件，并且把他们给分切开，并把它们分配给下游的reader。reader将会去读取具体的数据。切分后的每个数据块只被一个reader读取，但是一个reader可以读取多个被切分后的数据块，多次多取，一次只读取一个。

重点:

- 1.如果watchType被设置为FileProcessingMode.PROCESS_CONTINUOUSLY，当一个文件已经被监控时，那么该文件里的内容将会被重复处理。这将会破坏了"exactly-once"语义，追加到文件尾部的数据将会导致整个文件里的数据都会被重复处理。

- 2.如果watchType被设置为FileProcessingMode.PROCESS_ONCE，那么数据源将会只扫描路径一次病退出，不会等待reader读取完整个文件。当然，reader会一直读取文件，直到所有文件的内容都被读完。关闭source后将不会再有新的checkpoints。这将会导致节点数据恢复速度变慢。因为作业会从上个检查点继续读取数据。

### 基于Socket的数据源
- socketTextStream - 从socket中读取数据，每个元素都被分隔符分开。

### 基于Collection的数据源
- fromCollection(Collection) - 从Java Java.util.Collection来创建数据流，collection中的所有元素都必须是相同的类型才可以。

- fromCollection(Iterator, Class) - 从迭代器中创建一个数据流。class指定了迭代器返回的每个元素的类型。

- fromElements(T ...) - 从给的对象序列创建一个数据流，所有的对象都必须要是相同的类型。

- fromParallelCollection(SplittableIterator, Class) - 从迭代器中创建一个并行的数据流，class指定了迭代器返回的每个元素的类型。

- generateSequence(from, to) - 并行生成给定间隔中的数字序列。

### 自定义数据源
addSource - 添加新数据源的方法。例如，如果要从Apache Kafka中读取数据，你可以使用执行addSource(new FlinkKafkaConsumer08<>(...))方法。可以在 connectors 章节中查看更详细的描述。

## DataStream的转换操作
可以在operators章节里查看具体的转换操作。

## Data Sink
Data sink把DataStreams中的数据写入到文件、socket等外部系统中，或者打印它们。Flink的流计算自带了多种格式的输出操作，这些操作封装在DataStreams类中：

- writeAsText() / TextOutputFormat - 将元素作为字符串逐行写入。字符串是由数据中每个元素的toString()方法生成的。

- writeAsCsv(...) / CsvOutputFormat - 把元组写成CSV文件（逗号分隔）。行/字段分隔符是可配置的。每个字段的值都是由toString()字段里对象的toString()方法获得的。

- print() / printToErr() - 把流中每个元组的toString()值打印到标准输出流/标准错误流中。可选的，一个前缀会被添加到输出上。这可以区分不同调用打印的信息。如果并发度大于1，输出的打印信息将会标识出是被那个任务输出的。

- writeUsingOutputFormat() / FileOutputFormat - 自定义文件输出的方法和基类。支持自定义object-to-bytes的转换器。

- writeToSocket - 更具SerializationSchema把数据写入到socket中去。

- addSink - 执行自定义sink的函数。Flin可以通过绑定实现了sink函数的connectors到其它系统中去，例如Apache Kafka等等。

注意：DataStream上的write*()方法的主要目的是用于调试。他们没有参与Flink的checkpointing，这意味着这些方法通常只具备at-least-once语义。数据被写到目标系统的格式取决于OutputFormat的实现。这意味着并不是所有发送到OutputFormat的数据都能够立刻在目标系统中显示。同样的，在写失败的情况下，这些数据可能会丢失。

为了能够可靠的把数据流exactly-once写入到文件系统中，请使用connector来写入。此外，Flink通过调用.addSink(...)方法来添加自定义的Sink，使得Flink在写文件操作上具备exactly-once语义。

## Iterations

迭代流程序实现了一个步函数，并把它嵌入到了IterativeStream中。作为一个无限流计算的DataStream程序，最大迭代次数是无法确定的。相反，通过使用split或filter转换,你可以指定流的哪一部分用于反馈给迭代头（IterativeStream），哪一部分分发给下游。这里以我们以filter为例子来展示迭代流程序的AIP使用模式，首先我没定义一个IterativeStream：

- 下面是Java的例子

```
IterativeStream<Integer> iteration = input.iterate();
```

然后，我们指定一个在转换操作内部循环执行的逻辑，我们这里使用的是一个简单的map转换：

```
DataStream<Integer> iterationBody = iteration.map(/* this is executed many times */);
```

为了能够关闭迭代并定义迭代尾，需要需要调用IterativeStream的closeWith(feedbackStream)方法。传给closeWith方法的DataStream将会反馈给迭代头。通常的方法是使用filter方法来过滤部分数据流来反馈给迭代头，或给下游。

```
iteration.closeWith(iterationBody.filter(/* one part of the stream */));
DataStream<Integer> output = iterationBody.filter(/* some other part of the stream */);
```

例如，这里程序将会从一系列整数中减去1，直到它们达到零：

```
//定义一个数据流
DataStream<Long> someIntegers = env.generateSequence(0, 1000);

//定义一个迭代数据流
IterativeStream<Long> iteration = someIntegers.iterate();

//定义步函数（迭代操作），执行一次就是把值减一
DataStream<Long> minusOne = iteration.map(new MapFunction<Long, Long>() {
  @Override
  public Long map(Long value) throws Exception {
    return value - 1 ;
  }
});

//把值大于0的元素过滤出来
DataStream<Long> stillGreaterThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value > 0);
  }
});

//把过滤出来的stillGreaterThanZero返回给迭代头，stillGreaterThanZero过滤逻辑过滤出来的数据不再参与迭代
iteration.closeWith(stillGreaterThanZero);

//过滤出DataStream lessThanZero，lessThanZero可以继续参与其它流计算
DataStream<Long> lessThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value <= 0);
  }
});
```

- 下面是Scala的例子

```
val iteratedStream = someDataStream.iterate(
  iteration => {
    val iterationBody = iteration.map(/* this is executed many times */)
    (iterationBody.filter(/* one part of the stream */), iterationBody.filter(/* some other part of the stream */))
})
```

和之前的例子一样，这里程序将会从一系列整数中减去1，直到它们达到零：

```
val someIntegers: DataStream[Long] = env.generateSequence(0, 1000)

val iteratedStream = someIntegers.iterate(
  iteration => {
    val minusOne = iteration.map( v => v - 1)
    val stillGreaterThanZero = minusOne.filter (_ > 0)
    val lessThanZero = minusOne.filter(_ <= 0)
    (stillGreaterThanZero, lessThanZero)
  }
)
```

## 执行参数
StreamExecutionEnvironment 可以使用 ExecutionConfig 来设置作业运行时的配置。

更多参数说明，请参阅[execution configuration](https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/execution_configuration.html "execution configuration")。这里参数特别适合在DataStream中使用：

- setAutoWatermarkInterval(long milliseconds):设置自动水印发射规则，可以通过调用getAutoWatermarkInterval()来获取到当前设置的值。

### 故障转移

[State & Checkpointing](https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/checkpointing.html "State & Checkpointing") 描述了如何启用和配置Flink的检查点机制。

### 控制延迟

默认情况下，流中的数据没有一条条被处理，因为这样会造成不必要的网络开销，而是被缓存来处理的。缓存的大小可以在Flink的配置文件中配置。虽然这个方法可以优化吞吐量，但在流数据传输不够快的时候可能会产生延迟。为了控制吞吐量和延迟情况，你可以在运行环境中（或者单个数据操作上）调用env.setBufferTimeout(timeoutMillis)方法，来给buffer填充设置一个最大等待时间。超过这个时间后，buffers在没有被填充满的情况下也会自动的被发送。默认的超时时间是100ms。

Usage:

- Java

```
LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
env.setBufferTimeout(timeoutMillis);

env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis);
```

- Scala

```
val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment
env.setBufferTimeout(timeoutMillis)

env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis)
```

需要使用最大的吞吐量，可以取消buffer的延迟，只有当buffer被填充满的时候才会被刷走，设置setBufferTimeout(-1)即可。
需要最小延迟的话，可以设置延时尽可能的小，例如 5 或 10 ms。要避免设置超时时间为0，因为这会造成服务性能下降。


## 调试
在把Flink程序部署到集群中运行前，需要确认实现的算法逻辑是正确的。所以，实现数据处理的程序通常是检查结果、调试和改进的增量过程。

Flink提供了一些特性，通过支持从IDE内部进行本地调试、注入测试数据和收集结果数据，可以显著简化数据分析程序的开发过程。本节给出了一些如何简化Flink程序开发的说明。

### 本地执行环境
在创建LocalStreamEnvironment的JVM中启动Flink系统。如果在IDE中创建LocalStreamEnvironment，则可以在代码中设置断点并轻松调试程序。

下面是创建一个LocalEnvironment的例子:

- Java

```
final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

DataStream<String> lines = env.addSource(/* some source */);
// build your program

env.execute();
```

- Scala

```
val env = StreamExecutionEnvironment.createLocalEnvironment()

val lines = env.addSource(/* some source */)
// build your program

env.execute()
```

### Collection Data Sources

Flink提供了特殊的数据源，这些数据源由Java集合支持，以方便测试。 一旦程序需要被测试，可以简单的替换sources（读数据的地方）和sinks（写数据的地方）来实现。

集合数据源可以向下边这样使用：

- Java

```
final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

// Create a DataStream from a list of elements
DataStream<Integer> myInts = env.fromElements(1, 2, 3, 4, 5);

// Create a DataStream from any Java collection
List<Tuple2<String, Integer>> data = ...
DataStream<Tuple2<String, Integer>> myTuples = env.fromCollection(data);

// Create a DataStream from an Iterator
Iterator<Long> longIt = ...
DataStream<Long> myLongs = env.fromCollection(longIt, Long.class);
```

- Scala

```
val env = StreamExecutionEnvironment.createLocalEnvironment()

// Create a DataStream from a list of elements
val myInts = env.fromElements(1, 2, 3, 4, 5)

// Create a DataStream from any Collection
val data: Seq[(String, Int)] = ...
val myTuples = env.fromCollection(data)

// Create a DataStream from an Iterator
val longIt: Iterator[Long] = ...
val myLongs = env.fromCollection(longIt)
```

>注意：集合数据源里的数据类型和iterator都需要实现Serializable接口。而且，集合数据源不能够被并行的执行处理。

### Iterator Data Sink

Flink同时也为测试和调试的DataStream提供了一个sink。可以像下面这样使用它们：

- Java

```
import org.apache.flink.streaming.experimental.DataStreamUtils

DataStream<Tuple2<String, Integer>> myResult = ...
Iterator<Tuple2<String, Integer>> myOutput = DataStreamUtils.collect(myResult)
```

- Scala

```
import org.apache.flink.streaming.experimental.DataStreamUtils
import scala.collection.JavaConverters.asScalaIteratorConverter

val myResult: DataStream[(String, Int)] = ...
val myOutput: Iterator[(String, Int)] = DataStreamUtils.collect(myResult.javaStream).asScala
```

>注意：flink-streaming-contrib模块的代码已经从Flink 1.5.0开始被移除，相关的代码已经被移到flink-streaming-java 和 flink-streaming-scala模块中去。






